{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, lea</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro con usted puede ser diferente a la de otros cursos que haya tomado. Prefiero no escribir código mientras observa. En su lugar, ejecuto Jupyter Labs, como este, y le doy una idea de lo que está sucediendo. Le sugiero que lo haga usted mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una solicitud de cambio en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuerda siempre esta línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La API que ya conocemos\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic tiene una API ligeramente diferente y se requieren Max Tokens\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "`ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "`ollama ls` lista todos los modelos que has descargado\n",
    "`ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competencia entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"mejor competidor número\", \"segundo mejor competidor número\", \"tercer mejor competidor número\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usó esto? Intenta actualizar esto para añadir otro patrón de diseño Agente.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
