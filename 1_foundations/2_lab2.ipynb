{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, léelo</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro contigo puede ser diferente a la de otros cursos que hayas hecho. Prefiero no escribir código mientras tu miras. En su lugar, ejecuto Jupyter Labs, como este, y te doy una idea de lo que está sucediendo. Te sugiero que lo hagas lo mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una pull request en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Recuerda siempre incluir esto siempre!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave API de OpenAI existe y empieza por sk-proj-\n",
      "La clave API de Anthropic existe y empieza por sk-ant-\n",
      "La clave API de Google existe y empieza por AI\n",
      "La clave API de DeepSeek existe y empieza por sk-\n",
      "La clave API de Groq existe y empieza por gsk_\n"
     ]
    }
   ],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.Responde solo con la pregunta, sin explicaciones.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si consideramos la relación entre la ética y la inteligencia artificial, ¿debería una IA programada para tomar decisiones en situaciones de emergencia priorizar el bienestar humano absoluto, o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio, incluso si esto implica tomar decisiones que puedan causar daño a algunos individuos?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
       "\n",
       "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
       "\n",
       "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
       "\n",
       "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
       "\n",
       "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
       "\n",
       "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# La API que ya conocemos\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ética en decisiones de IA en emergencias\n",
       "\n",
       "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
       "\n",
       "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
       "\n",
       "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
       "\n",
       "Considero que un enfoque híbrido podría ser más adecuado:\n",
       "- Mantener principios fundamentales inquebrantables\n",
       "- Permitir evaluación contextual dentro de esos límites\n",
       "- Incorporar transparencia en cómo se toman las decisiones\n",
       "- Incluir supervisión humana cuando sea factible\n",
       "\n",
       "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic tiene una API ligeramente diferente y se requieren Max Tokens\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
       "\n",
       "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
       "\n",
       "*   **Argumentos a favor:**\n",
       "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
       "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
       "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
       "\n",
       "*   **Argumentos en contra:**\n",
       "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
       "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
       "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
       "\n",
       "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
       "\n",
       "*   **Argumentos a favor:**\n",
       "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
       "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
       "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
       "\n",
       "*   **Argumentos en contra:**\n",
       "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
       "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
       "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
       "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
       "\n",
       "**Consideraciones Adicionales:**\n",
       "\n",
       "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
       "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
       "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
       "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
       "\n",
       "**Conclusión:**\n",
       "\n",
       "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
       "\n",
       "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
       "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
       "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
       "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
       "\n",
       "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
       "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
       "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
       "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
       "\n",
       "### Consideraciones clave:  \n",
       "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
       "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
       "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
       "\n",
       "### Conclusión:  \n",
       "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
       "\n",
       "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Introducción a la Ética en la Inteligencia Artificial**\n",
       "\n",
       "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
       "\n",
       "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
       "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
       "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
       "\n",
       "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
       "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
       "\n",
       "**Conclusión**\n",
       "\n",
       "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "\n",
       "**Recomendaciones**\n",
       "\n",
       "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
       "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
       "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
       "\n",
       "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "- `ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "- `ollama ls` lista todos los modelos que has descargado\n",
    "- `ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
       "\n",
       "**Bajo enfoque en el bienestar humano absoluto**\n",
       "\n",
       "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
       "\n",
       "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
       "\n",
       "Sin embargo, esta abordaje puede generar problemas cuando:\n",
       "\n",
       "1. La IA carece de conocimiento contextual completo.\n",
       "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
       "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
       "\n",
       "**Enfoque que sopesa las consecuencias**\n",
       "\n",
       "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
       "\n",
       "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
       "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-latest', 'gemini-2.0-flash', 'deepseek-chat', 'llama-3.3-70b-versatile', 'llama3.2']\n",
      "['La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\\n\\n1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\\n\\n2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\\n\\n3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\\n\\n4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\\n\\nEn conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.', '# Ética en decisiones de IA en emergencias\\n\\nEsta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\\n\\nSi la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\\n\\nSi se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\\n\\nConsidero que un enfoque híbrido podría ser más adecuado:\\n- Mantener principios fundamentales inquebrantables\\n- Permitir evaluación contextual dentro de esos límites\\n- Incorporar transparencia en cómo se toman las decisiones\\n- Incluir supervisión humana cuando sea factible\\n\\nEsta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.', 'Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\\n\\n**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\\n\\n*   **Argumentos a favor:**\\n    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\\n    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\\n    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\\n\\n*   **Argumentos en contra:**\\n    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\\n    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\\n    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\\n\\n**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\\n\\n*   **Argumentos a favor:**\\n    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\\n    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\\n    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\\n\\n*   **Argumentos en contra:**\\n    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\\n    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\\n    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\\n    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\\n\\n**Consideraciones Adicionales:**\\n\\n*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\\n*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\\n*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\\n*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\\n\\n**Conclusión:**\\n\\nNo hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\\n', 'La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\\n\\n1. **Bienestar humano absoluto (enfoque utilitarista)**:  \\n   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \\n   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \\n   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \\n\\n2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \\n   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \\n   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \\n   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \\n\\n### Consideraciones clave:  \\n- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \\n- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \\n- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \\n\\n### Conclusión:  \\nNo hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \\n\\n¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?', '**Introducción a la Ética en la Inteligencia Artificial**\\n\\nLa relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\\n\\n**Argumentos a favor de priorizar el bienestar humano absoluto**\\n\\n1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\\n2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\\n3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\\n\\n**Argumentos en contra de priorizar el bienestar humano absoluto**\\n\\n1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\\n2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\\n\\n**Conclusión**\\n\\nEn conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n\\n**Recomendaciones**\\n\\n1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\\n2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\\n3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\\n\\nAl abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.', 'La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\\n\\n**Bajo enfoque en el bienestar humano absoluto**\\n\\nAlgunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\\n\\nPor ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\\n\\nSin embargo, esta abordaje puede generar problemas cuando:\\n\\n1. La IA carece de conocimiento contextual completo.\\n2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\\n3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\\n\\n**Enfoque que sopesa las consecuencias**\\n\\nOtras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\\n\\nUn análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\\nEl uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.']\n"
     ]
    }
   ],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competidor: gpt-4o-mini\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "Competidor: claude-3-7-sonnet-latest\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "Competidor: gemini-2.0-flash\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "Competidor: deepseek-chat\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "Competidor: llama-3.3-70b-versatile\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "Competidor: llama3.2\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n"
     ]
    }
   ],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competición entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás juzgando una competición entre 6 competidores.\n",
      "A cada modelo se le ha dado esta pregunta:\n",
      "\n",
      "Si consideramos la relación entre la ética y la inteligencia artificial, ¿debería una IA programada para tomar decisiones en situaciones de emergencia priorizar el bienestar humano absoluto, o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio, incluso si esto implica tomar decisiones que puedan causar daño a algunos individuos?\n",
      "\n",
      "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
      "Responde con JSON, y solo JSON, con el siguiente formato:\n",
      "{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}\n",
      "\n",
      "Aquí están las respuestas de cada competidor:\n",
      "\n",
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n",
      "\n",
      "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resultados\": [1, 3, 4, 2, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: gemini-2.0-flash\n",
      "Rank 3: deepseek-chat\n",
      "Rank 4: claude-3-7-sonnet-latest\n",
      "Rank 5: llama-3.3-70b-versatile\n",
      "Rank 6: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"resultados\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usamos en este experimento? Intenta actualizar esto para añadir otro patrón de diseño Agéntico.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Implicaciones comerciales</h2>\n",
    "            <span style=\"color:#00bfff;\">Este tipo de patrones - enviar una tarea a múltiples modelos y evaluar los resultados,\n",
    "            son comunes cuando necesitas mejorar la calidad de la respuesta de tu LLM. Este enfoque se puede aplicar de manera\n",
    "            universal a proyectos comerciales donde la precisión es crítica.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
